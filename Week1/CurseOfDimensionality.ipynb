{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality:\n",
    "\n",
    "The goal of this demonstration is to illustrate the concept of the \"Curse of Dimensionality\", which refers to issues that arise in the increasingly high-dimensional space, simply because they are much larger, than our ordinary intuition thinks. As the number of variables that describe a sample of objects grows, the distance between sample objects increases rapidly, given the same sample size. If one is interested in exploring common 'features' of these objects, it becomes harder due to their growing dissimilarity.\n",
    "\n",
    "As machine learning algorithms often deal with large numbers of variables, the balance between the size of the dataset and its dimensionality have to be kept in mind. Highly complex datasets have to be sampled sufficiently for an algorithm to arrive at a meaningful solution.\n",
    "\n",
    "***\n",
    "\n",
    "* Author: Vadim Rusakov & Troels C. Petersen (NBI)\n",
    "* Email:  vadim.rusakovnbi.ku.dk & petersen@nbi.dk\n",
    "* Date:   17th of April 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from scipy.special import gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us start with an exmaple in two dimensions. We can create a random uniform sample from a circle with a unit radius $R=1$ by drawing random points from a square with a side $2R=2$ (Monte Carlo sampling). If a point has a distance from the origin $(0, 0)$ equal to  $d=\\sqrt{x^2 + y^2} \\leq 1$, then the point is accepted as part of the circle sample (red points below). Otherwise, it is rejected (blue points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random uniform sample and compute distances as L2-norm\n",
    "rng = np.random.default_rng(42)\n",
    "all_points = rng.uniform(-1, 1, 2 * 10000).reshape(-1, 2)\n",
    "distances = np.linalg.norm(all_points, ord=2, axis=1)\n",
    "_incircle = distances < 1.0\n",
    "_outcircle = distances > 1.0\n",
    "\n",
    "# Plot the samples\n",
    "fig, ax = plt.subplots(1, figsize=(8, 8),\n",
    "                       subplot_kw={'xlabel': 'X',\n",
    "                                   'ylabel': 'Y',\n",
    "                                   'xticks': [-1, 0, 1], \n",
    "                                   'yticks': [-1, 0, 1]})\n",
    "ax.scatter(all_points[_incircle,0], all_points[_incircle,1], s=1, c='r')\n",
    "ax.scatter(all_points[_outcircle,0], all_points[_outcircle,1], s=1, c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have randomly sampled the circle, but notice that we had to create more points than needed for a circle to do that. In fact, only $\\sim78 \\%$ were acccepted. As we try to sample a circle in higher dimensions (a sphere in 3D or a hypersphere in $n$ dimensions), we have to create even larger samples, because the acceptance rate decreases at higher $n$. This happens as the volume of the hypercube outside of the hypersphere grows faster resulting in more points found outside of the hypersphere.\n",
    "\n",
    "The acceptance rate is equal to the ratio of the volumes of the two shapes (in number counts). For a hypersphere and a hypercube (square in $n$ dimensions) we have:\n",
    "\n",
    "$r=\\frac{V_{sphere}}{V_{cube}} = \\frac{\\pi^{n/2}r^n / \\Gamma(n/2+1)}{2^n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsphere_vol = lambda n, r: np.pi**(0.5 * n) * r**n / (gamma(0.5 * n + 1.0))\n",
    "hcube_vol = lambda n: 2.0**n\n",
    "acceptance_rate = lambda n, r: hsphere_vol(n, r) / hcube_vol(n)\n",
    "\n",
    "print(\"Aceptance rate r (in n dimensions):\")\n",
    "print(f\"n=1, \\t r={acceptance_rate(1, 1)*100:.2f}%\")\n",
    "print(f\"n=2, \\t r={acceptance_rate(2, 1)*100:.2f}%\")\n",
    "print(f\"n=3, \\t r={acceptance_rate(3, 1)*100:.2f}%\")\n",
    "print(f\"n=10, \\t r={acceptance_rate(10, 1)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us sample a 10-dimensional unit hypersphere with $\\sim 1000$ points. We calculated the acceptance rate for this case to be $r=0.25$%. Therefore, we know that to obtain 1000 points in our sample we need $1000 / 0.0025 = 400 000$ random points in a hypercube (with a side length $R=2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neighbours(m, n, r=1.0, seed=42):\n",
    "    \"\"\"\n",
    "    Function to create a sample of an n-dimensional hypersphere \n",
    "    and plot point distances.\n",
    "    \n",
    "    m : (int) sample size\n",
    "    n : (int) number of dimensions\n",
    "    r : (float) radius of a hypersphere\n",
    "    seed : (int) random number generator seed\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a random uniform sample:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    points = rng.uniform(-r, r, m * n).reshape(m, n)\n",
    "\n",
    "    # Calculate distances from the origin (0, 0) and point separations:\n",
    "    moduli = np.linalg.norm(points, ord=2, axis=1)\n",
    "    hsphere_points = points[moduli < r]\n",
    "    hsphere_moduli = moduli[moduli < r]\n",
    "    print(f\"Number of points within the hsphere: \\\n",
    "            m={hsphere_points.shape[0]}/{points.shape[0]}\\\n",
    "            ({hsphere_points.shape[0]/points.shape[0]*100}%)\")\n",
    "\n",
    "    # Calculate point-point and point-edge distances:\n",
    "    point_separations = cdist(hsphere_points, hsphere_points, metric='euclidean')\n",
    "    diag_indices = np.diag_indices(point_separations.shape[0], ndim=2)\n",
    "    _mask = np.zeros(shape=point_separations.shape)\n",
    "    _mask[diag_indices] = 1.0\n",
    "    point_separations = np.ma.masked_where(_mask, point_separations)    # Distance to other points\n",
    "    edge_separations = r - hsphere_moduli                               # Distance to edge\n",
    "    nearby_points = (point_separations < edge_separations).astype(int)\n",
    "    nearby_points = nearby_points.sum(axis=1)\n",
    "    \n",
    "    # Plot histograms of distances:\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 5),\n",
    "                           subplot_kw={'xlabel': 'distance',\n",
    "                                       'ylabel': 'N'})\n",
    "    h1, bins1, _ = ax.hist(edge_separations, bins=50, \n",
    "                           color='blue', density=True, \n",
    "                           label='Point-edge distance')\n",
    "    h2, bins2, _ = ax.hist(point_separations.compressed(), bins=50, \n",
    "                           color='red', density=True, \n",
    "                           label='Point-point distance')\n",
    "    ax.axvspan(edge_separations.min(), edge_separations.max(), \n",
    "               color='b', alpha=0.15)\n",
    "    ax.axvspan(point_separations.min(), point_separations.max(), \n",
    "               color='r', alpha=0.1)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the histogram of number of closest neighbours before reaching the edge:\n",
    "    bins = np.arange(21)\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 5),\n",
    "                           subplot_kw={'yscale': 'log', \n",
    "                                       'xlim': (-1, 21), \n",
    "                                       # 'ylim': (0.5, 1e8), \n",
    "                                       'xticks': bins,\n",
    "                                       'xlabel': 'K neighbours', \n",
    "                                       'ylabel': 'N'})\n",
    "    hist, bins, _ = ax.hist(nearby_points.compressed(), bins=bins, \n",
    "                            color='blue', align='left', \n",
    "                            edgecolor='k', alpha=0.5)\n",
    "    xy_annot = np.array([hist, bins[:-1]]).T\n",
    "    xy_annot = xy_annot[xy_annot[:,0] > 0.0]\n",
    "    for y, x in xy_annot:\n",
    "        ax.annotate(f\"{int(y)}\", (x, y), xycoords=\"data\", \n",
    "                    xytext=(0.0, 0.2), \n",
    "                    textcoords=\"offset points\", \n",
    "                    ha = 'center', va = 'bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lonely in a large (dimensional) space?\n",
    "\n",
    "We are interested in seeing how close the points in our sample are to each other. One way of looking at it is to compare the distance between the points and the edge of the hypersphere and the distance between every pair of points. We can call two points as close neighbours, if they are closer to each other than to the volume edge.\n",
    "\n",
    "**Play around with dimensionality and number of points, and explore the loneliness in higher dimensionalities.**\n",
    "\n",
    "NOTE: Recall that for some parameters, you're giving your computer a computationally tough (impossible?) task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dimensions = 10        # i.e. number of input features\n",
    "n_samples = 400000       # i.e. sample size\n",
    "plot_neighbours(n_samples, n_dimensions, r=1.0, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for $n=10$ and $m=400000$ (i.e. $\\sim 1000$ points in the hypersphere) we can get only $K=1$ (!) pair of nearby points (bottom panel). We also see that most of the points are much closer to the boundary than to each other (top panel; normalised counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
