{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction 2\n",
    "\n",
    "__Week 2 - 4 May 2022__\n",
    "\n",
    "After we have practised using some dimensionality reduction algorithms in basic mode, we will now turn our attention to arguably the most important part of the process - preprocessing and hyperparameter optimisation. Hopefully this will show how these steps can be crucial in producing the best possible representation of data in lower dimensions. Here, we will make use of a new astronomical dataset with the same algorithms as before (`PCA`, `t-SNE`, `UMAP`). We will end by returning to galaxies in COSMOS2015 and improving on the results from the previous session.\n",
    "\n",
    "---\n",
    "### Data\n",
    "\n",
    "Swift dataset - the dataset is light curves (intensity of light on a timeline) of gamma-ray bursts (GRB) coming from the far reaches of the universe. In addition to the raw data, there are several derived properties:\n",
    "\n",
    "* `T90_start` & `T90_end` - start and end times of the bursts of light.\n",
    "* `fluence`\n",
    "* `hardness`\n",
    "\n",
    "---\n",
    "### Exercise\n",
    "\n",
    "You have now practiced reducing a dataset to a set of basis vectors. The next step is to take an additional step of considering which data it is best to use in the first place and how to prepare it.\n",
    "\n",
    "* Apply your favourite dimensionality reduction algorithm to: (1) properties, (2) raw observed data, (3) preprocessed observed data in the Swift dataset.\n",
    "\n",
    "* Tweak hyperparameters of the algorithm - can you find an optimal combination?\n",
    "\n",
    "* P.S. Why use only one of the algorithms at a time? Think about using one of these for preprocessing and the other for dimensionality reduction (eg., `PCA` + `t-SNE`).\n",
    "\n",
    "\n",
    "---\n",
    "* Authors:  Vadim Rusakov, Charles Steinhardt, Jackson Mann\n",
    "* Email:  vadim.rusakov@nbi.ku.dk\n",
    "* Date:   27th of April 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - umap-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.11.28         |           py37_0         148 KB  conda-forge\n",
      "    umap-learn-0.4.6           |   py37hc8dfbb8_0         110 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         259 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  umap-learn         conda-forge/osx-64::umap-learn-0.4.6-py37hc8dfbb8_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi                                         pkgs/main --> conda-forge\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the raw data (light curves), as well as the properties (derived form raw data). \n",
    "\n",
    "Make sure to get rid of the `NaN` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data (light curves)\n",
    "data = pd.read_pickle('datasets/swift_gamma_ray_bursts.zip')\n",
    "\n",
    "# loading derived properties\n",
    "props_full = pd.read_csv('datasets/SwiftProperties.csv')\n",
    "\n",
    "# match & merge properties and raw data tables\n",
    "df_merged = pd.merge(props_full, data, left_on='GRBname', right_index=True, how='inner')\n",
    "mask = ~np.isnan(df_merged.iloc[:, 1:]).any(axis=1) # mask of NaNs\n",
    "df_clean = df_merged.loc[mask] # clean of NaNs\n",
    "X_data = df_merged.values[mask, 7:] # raw data\n",
    "X_props = df_merged.values[mask, :7] # derived properties\n",
    "\n",
    "print(f\"the shape of the raw data set (nsamples, nfeatures): {X_data.shape}\")\n",
    "print(f\"the shape of the properties data set (nsamples, nfeatures): {X_props.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1. Gamma-ray bursts\n",
    "\n",
    "1. Run your favourite algorithm (`t-SNE` or `UMAP`) on the bursts properties (`X_props`). Does it manage to split the samples into two types: eg., short and long bursts? $^1$\n",
    "\n",
    "2. Can you explain the clumpiness of the reduction? Based on this, do derived prperties give a meaningful reduction? In not, can you argue why?\n",
    "\n",
    "\n",
    "$^1$ Hint: short- or long-type of bursts separation is not obvious. Calculate the duration of bursts using the properties table and by subtracting `T90_start` from `T90_end`. Colour your scatter plots in reduced dimensions by duration. Try other properties as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running t-SNE\n",
    "tsne = TSNE(perplexity=15, n_iter=1500, init='random', verbose=2, \n",
    "            learning_rate='auto', method='barnes_hut', random_state=42)\n",
    "y = tsne.fit_transform(X_props[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 5), dpi=100)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "\n",
    "duration = np.log10(df_clean['T90_end'] - df_clean['T90_start'])\n",
    "\n",
    "sc = ax.scatter(y[:, 0], y[:, 1], s=2.0, norm=LogNorm(), \n",
    "                c=duration, cmap='jet')\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.ax.set_ylabel('Duration', rotation=270, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reapeat with `UMAP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = umap.UMAP(n_components=2, n_neighbors=50, random_state=42)\n",
    "y = map.fit_transform(X_props[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 5), dpi=100)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "\n",
    "sc = ax.scatter(y[:, 0], y[:, 1], s=1.0, norm=LogNorm(), \n",
    "                c=duration,  cmap='jet')\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.ax.set_ylabel('Duration', rotation=270, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now train the algorithm with the raw data `X_data`. Are you able to make cleaner classifications using the projections in the space learnt by the algorithms? If not, do you have some ideas for how to improve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = umap.UMAP(n_components=2, n_neighbors=50, random_state=42)\n",
    "y = map.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 5), dpi=100)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "\n",
    "duration = np.log10(df_clean['T90_end'] - df_clean['T90_start'])\n",
    "\n",
    "sc = ax.scatter(y[:, 0], y[:, 1], s=2.0, norm=LogNorm(), \n",
    "                c=duration, cmap='jet')\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.ax.set_ylabel('Duration', rotation=270, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suggestion: try converting the time-series `X_data` to frequency space by taking a Fourier transform. Repeat the exercise. Does this help to create a cleaner representation in the reduced space? What physical meaning does the separation imply? (for eg., in terms of the burst duration property)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking Fast Fourier Transform\n",
    "X_data_fft = abs(np.fft.rfft(X_data)) # take modulus to convert to float from complex data type\n",
    "mask_fft = ~np.isnan(X_data_fft[:, 1:]).any(axis=1) # NaNs\n",
    "X_data_mod = X_data_fft[mask_fft] # mask out NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running t-SNE\n",
    "tsne = TSNE(perplexity=15, n_iter=1500, init='random', verbose=2, \n",
    "            learning_rate='auto', method='barnes_hut', random_state=42)\n",
    "y = tsne.fit_transform(X_data_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 5), dpi=100)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "duration = np.log10(df_clean['T90_end'] - df_clean['T90_start'])\n",
    "duration = duration.iloc[mask_fft]\n",
    "\n",
    "sc = ax.scatter(y[:, 0], y[:, 1], s=2.0, norm=LogNorm(), \n",
    "                c=duration, cmap='jet')\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.ax.set_ylabel('Duration', rotation=270, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Try experimenting: the main parameter in `t-SNE` is `perplexity` - try using the values of `5, 10, 15, 20, 50`. The analogous parameter for `UMAP` is `n_neighbours`. Can you find a number that makes an optimal separation of bursts? How would this number have to change depending on the size of the dataset, eg. if you have **10x**, **100x** more samples?\n",
    "\n",
    "6. Try changing the different options `method='exact'` $\\left[ \\mathcal{O}(n^2) \\right]$ and `method='barnes_hut'` $\\left[ \\mathcal{O}(n \\log{n}) \\right]$. The complexity of the operations with the two is different: `barnes_hut` provides an approximate map, but with less time spent.\n",
    "\n",
    "7. Try different value of the number of iterations (`n_iter`). Can you reach the point where the algorithm converges (be gentle to your computer)? Remember that you need to have at least `n_iter = 250`, as the algorithm requires those for calibration. If it takes too long to converge try taking a smaller sub-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to COSMOS2015 (galaxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, repeat the experiment from the previous session with the galaxies dataset. This time think about preprocessing and hyperparameter optimisation and repeat the exercise. At the end try to classify galaxies as either `alive` or `dead`.\n",
    "\n",
    "1. Recall that the linear `PCA` before was not capable of linearly separating two types of galaxies at all. In fact, the data you were given were not entirely ready. You need to preprocess them first. For this, take all fluxes and normalize them by one of the other fluxes. $^{1,2}$ Repeat the exercise. Do you get better separation?\n",
    "\n",
    "$^1$ Hint: it turns out the star forming and dead galaxies are most different in the bluer bands (like `u_flux`).\\\n",
    "$^2$ Hint: plot distributions of features before preprocessing and compare with the same after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off warnings, no one needs them...\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "file = \"datasets/cosmos2015.csv\"\n",
    "df = pd.read_csv(file, index_col=False)\n",
    "\n",
    "# select a random sub-sample of the dataset\n",
    "n = 10000\n",
    "idxs = np.arange(df.shape[0])\n",
    "idxs_rand = np.random.choice(idxs, size=n)\n",
    "df_cut = df.iloc[idxs_rand] # dataframe\n",
    "X = df.iloc[idxs_rand].values # array\n",
    "\n",
    "flux_cols = list(df.columns[4:]) # flux column names\n",
    "flux_idxs = np.argwhere(np.isin(df.columns, flux_cols)).flatten() # flux column indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "for i, col in enumerate(flux_cols):\n",
    "    vec = X[:, i]\n",
    "    vec = vec[(vec > (-10)) & (vec < 60)]\n",
    "    ax.hist(vec, bins=100, label=col, density=True, histtype='step')\n",
    "ax.set_ylim(0, 0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize fluxes \n",
    "norm_flux = df_cut.loc[:, 'u_flux'].values\n",
    "X_trans = np.copy(X)\n",
    "for col in flux_idxs:\n",
    "    X_trans[:, col] = X_trans[:, col] / norm_flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the normalization is below. The distributions are a little more closely spaced and span the similar numerical ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "for i, col in enumerate(flux_cols):\n",
    "    vec = X_trans[:, i]\n",
    "    vec = vec[(vec > (-10)) & (vec < 30)]\n",
    "    ax.hist(vec, bins=100, label=col, density=True, histtype='step')\n",
    "ax.set_ylim(0, 0.1)\n",
    "ax.set_xlim(-10, 60)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "y_pcs = pca.fit_transform(X_trans[:, flux_idxs]) # train only on fluxes (raw observed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 5), dpi=100)\n",
    "ax.set_xlim(np.percentile(y_pcs[:,0], 99), np.percentile(y_pcs[:,0], 1))\n",
    "ax.set_ylim(np.percentile(y_pcs[:,1], 99), np.percentile(y_pcs[:,1], 1))\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "is_sf = np.isin(df_cut.loc[:, 'is_star_forming'], 1)\n",
    "\n",
    "ax.scatter(y_pcs[is_sf, 0], y_pcs[is_sf, 1], s=0.02, c='b', norm=LogNorm())\n",
    "ax.scatter(y_pcs[~is_sf, 0], y_pcs[~is_sf, 1], s=0.02, c='r', norm=LogNorm())\n",
    "ax.annotate(\"star forming\", xy=(0.05, 0.9), xycoords=\"axes fraction\", \n",
    "            color='b', fontsize=12)\n",
    "ax.annotate(\"dead\", xy=(0.05, 0.86), xycoords=\"axes fraction\", \n",
    "            color='r', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='random', method='barnes_hut',   \n",
    "                     random_state=0, perplexity=10, n_iter=400, verbose=2)\n",
    "y = tsne.fit_transform(X_trans[:, flux_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 5), dpi=100)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "is_sf = np.isin(df_cut.loc[:, 'is_star_forming'], 1)\n",
    "\n",
    "ax.scatter(y[is_sf, 0], y[is_sf, 1], s=0.05, c='b', norm=LogNorm())\n",
    "ax.scatter(y[~is_sf, 0], y[~is_sf, 1], s=0.05, c='r', norm=LogNorm())\n",
    "ax.annotate(\"star forming\", xy=(0.05, 0.9), xycoords=\"axes fraction\", \n",
    "            color='b', fontsize=12)\n",
    "ax.annotate(\"dead\", xy=(0.05, 0.86), xycoords=\"axes fraction\", \n",
    "            color='r', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = umap.UMAP(n_components=2, random_state=42)\n",
    "y = map.fit_transform(X_trans[:, flux_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 5), dpi=100)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "is_sf = np.isin(df_cut.loc[:, 'is_star_forming'], 1)\n",
    "\n",
    "ax.scatter(y[is_sf, 0], y[is_sf, 1], s=0.05, c='b', norm=LogNorm())\n",
    "ax.scatter(y[~is_sf, 0], y[~is_sf, 1], s=0.05, c='r', norm=LogNorm())\n",
    "ax.annotate(\"star forming\", xy=(0.05, 0.9), xycoords=\"axes fraction\", \n",
    "            color='b', fontsize=12)\n",
    "ax.annotate(\"dead\", xy=(0.05, 0.86), xycoords=\"axes fraction\", \n",
    "            color='r', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Even with a high degree of separation, whether star forming or dead, there is a continuum of galaxies with varying masses and rates of star formation. Therefore, do you ever expect there to be discrete islands of galaxies when you reduce the dimensions? $^1$\n",
    "\n",
    "$^1$ Hint: produce scatter plots in the reduced space and colour the points by different galaxy properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + UMAP\n",
    "\n",
    "4. Try experimenting: can you combine `PCA` (or `kPCA`) and `t-SNE` as part of the preprocessing and training procedures? Do you get better results?\n",
    "\n",
    "5. Apply the new manifold-learning techniques to the old COSMOS2015 dataset. Now can you tell galaxies alive from dead better? Colouring the points by galaxy properties, what can you learn about active or dead galaxies (eg., which masses or rates of star formation do they tend to have)?\n",
    "\n",
    "Let us try to use a combination of PCA for preprocessing and UMAP for further reducing dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "kpca = KernelPCA(n_components=8, kernel='poly')\n",
    "y_pcs = kpca.fit_transform(X_trans[:, flux_idxs])\n",
    "\n",
    "# reducing dimensions to 2 components\n",
    "map = umap.UMAP(n_components=2, random_state=42)\n",
    "y = map.fit_transform(y_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 5), dpi=100)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "is_sf = np.isin(df_cut.loc[:, 'is_star_forming'], 1)\n",
    "\n",
    "ax.scatter(y[is_sf, 0], y[is_sf, 1], s=0.05, c='b', norm=LogNorm())\n",
    "ax.scatter(y[~is_sf, 0], y[~is_sf, 1], s=0.05, c='r', norm=LogNorm())\n",
    "ax.annotate(\"star forming\", xy=(0.05, 0.9), xycoords=\"axes fraction\", \n",
    "            color='b', fontsize=12)\n",
    "ax.annotate(\"dead\", xy=(0.05, 0.86), xycoords=\"axes fraction\", \n",
    "            color='r', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5875523c0f295e9a7a392db41a9a428ae2dda35a82a766a28c29c1f8950fbf4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
