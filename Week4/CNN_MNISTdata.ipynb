{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to a Convolutional NN - classifying handwritten numbers:\n",
    "\n",
    "For a long time, images were inaccesible to computers and ML algorithms. However, at the turn of the century very fast development - a slow revolution - happened, and the usage of Convolutional Neural Networks (CNNs) became computationally feasible. The advent of Graphical Processing Units (GPUs) also help propel the sharp rise in ML image analysis capabilities.\n",
    "\n",
    "Through first the MNIST database (1998-2004), and then the PASCAL (2005-2010) and ImageNet (2010-2017) competitions, CNN code developed to become capable of classifying the content of images. The following is an exercise, which uses the famous MNIST dataset, containing about 70000 images of handwritten digits, reduced (and anti-aliased) to 28-by-28 pixel black-and-write images.\n",
    "\n",
    "The CNN applied filters to these images, and then concatenates (i.e. \"boils down\") the images to lower resolution. This process is repeated, and finally the resulting pixel values are fed to a normal Neural Network, and made to provide an output, which lowers the loss function (in this case getting the value of the digit right). Through backpropagation, the filter values and the NN parameters are optimised, and thus in the end, the CNN becomes capable of evaluating images.\n",
    "\n",
    "The code and comments below is meant to be part illustration, part exercise. Note that it takes a few minutes to run (unless you have a GPU).\n",
    "\n",
    "***\n",
    "\n",
    "Authors: Carl Johnsen & Troels Petersen\n",
    "\n",
    "Emails: cjjohnsen@nbi.ku.dk & petersen@nbi.dk\n",
    "\n",
    "Date: 13th of May 2022 (latest version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Keras documentation can be found on keras.io:\n",
    "import keras\n",
    "from keras.datasets import mnist      # NOTE: This loads the data!\n",
    "# manual link to data:  http://yann.lecun.com/exdb/mnist/\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and testval sets\n",
    "(train_images, train_labels), (testval_images, testval_labels) = mnist.load_data()\n",
    "\n",
    "# further split testval set into specific test and validation set\n",
    "# the test set is NOT used during any part but inference\n",
    "val_images, test_images, val_labels, test_labels = train_test_split(testval_images, testval_labels, test_size=0.2, random_state=13052020)\n",
    "\n",
    "# explicitly illustrating standardization\n",
    "def standardizeimg(img, mu, sigma):\n",
    "    return (img-mu)/(sigma).astype(np.float32)\n",
    "\n",
    "# save for scaling test data\n",
    "mu_train = np.mean(train_images)\n",
    "sigma_train = np.std(train_images)\n",
    "\n",
    "# Standardize pixel distribution to have zero mean and unit variance\n",
    "train_images = standardizeimg(img=train_images, mu=mu_train, sigma=sigma_train)\n",
    "val_images = standardizeimg(img=val_images, mu=np.mean(val_images), sigma=np.std(val_images))\n",
    "\n",
    "# adapt to format required by tensorflow; Using channels_last --> (n_samples, img_rows, img_cols, n_channels)\n",
    "img_rows, img_cols = 28, 28 # input image dimensions\n",
    "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
    "val_images = val_images.reshape(val_images.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# convert class vectors to binary class matrices - one hot encoding\n",
    "num_classes = 10\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "val_labels = keras.utils.to_categorical(val_labels, num_classes)\n",
    "\n",
    "# avoid using statistics intrinsic to test data to ensure unbiased estimate of real model performance\n",
    "test_images = standardizeimg(img=test_images, mu=mu_train, sigma=sigma_train)\n",
    "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", train_images.shape)\n",
    "print(\" > labels:\", train_labels.shape)\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", val_images.shape)\n",
    "print(\" > labels:\", val_labels.shape)\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", test_images.shape)\n",
    "print(\" > labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify first 5 mages in each split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(10,6))\n",
    "for i in range(5):\n",
    "\n",
    "    # train\n",
    "    ax[0,i].imshow(train_images[i].reshape(28,28), cmap=plt.cm.binary)\n",
    "    ax[0,i].set_xlabel(np.argmax(train_labels[i]), fontsize=18)\n",
    "    ax[0,i].set_xticks([]); ax[0,i].set_yticks([]); ax[0,i].grid(False)\n",
    "    # val\n",
    "    ax[1,i].imshow(val_images[i].reshape(28,28), cmap=plt.cm.binary)\n",
    "    ax[1,i].set_xlabel(np.argmax(val_labels[i]), fontsize=18)\n",
    "    ax[1,i].set_xticks([]); ax[1,i].set_yticks([]); ax[1,i].grid(False)\n",
    "    # test\n",
    "    ax[2,i].imshow(test_images[i].reshape(28,28), cmap=plt.cm.binary)\n",
    "    ax[2,i].set_xlabel(np.argmax(test_labels[i]), fontsize=18)\n",
    "    ax[2,i].set_xticks([]); ax[2,i].set_yticks([]); ax[2,i].grid(False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for (un)balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(np.argmax(train_labels, axis=1).flatten()); ax[0].set_title('train', fontsize=18); ax[0].set_xticks(np.arange(10)); ax[0].tick_params(axis='both', which='major', labelsize=16);\n",
    "ax[1].hist(np.argmax(val_labels, axis=1).flatten()); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks(np.arange(10)); ax[1].tick_params(axis='both', which='major', labelsize=16);\n",
    "ax[2].hist(np.argmax(test_labels, axis=1).flatten()); ax[2].set_title('test', fontsize=18); ax[2].set_xticks(np.arange(10)); ax[2].tick_params(axis='both', which='major', labelsize=16);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create architecture\n",
    "\n",
    "See if you can also draw the below architecture, making it clear to others what exactly you've done/used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_rows, img_cols, 1)))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2, strides=None))\n",
    "\n",
    "model.add(Conv2D(filters=64,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2, strides=None))\n",
    "\n",
    "model.add(Dropout(rate=0.40))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify optimization strategy and metric used for monitoring during training\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "# the history object will contain a record of loss and metric values during training\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=128,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect learned kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers will print a list of layer parameters/values\n",
    "filters1, biases1 = model.layers[0].get_weights()\n",
    "filters2, biases2 = model.layers[2].get_weights()\n",
    "\n",
    "# normalize filter values to range 0-1 for better colormapping during plotting\n",
    "def norm_filter(kernel):\n",
    "    return (kernel - np.min(kernel)) / (np.max(kernel) - np.min(kernel))\n",
    "\n",
    "print('1st convolution layer:')\n",
    "fig, axs = plt.subplots(2,5, figsize=(10, 6))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].imshow(norm_filter(filters1[:,:,0,i]), cmap=plt.cm.binary)\n",
    "    axs[i].set_xticks([]); axs[i].set_yticks([]); axs[i].grid(False)\n",
    "plt.show()\n",
    "\n",
    "print('2nd convolution layer:')\n",
    "fig, axs = plt.subplots(2,5, figsize=(10, 6))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].imshow(norm_filter(filters2[:,:,0,i]), cmap=plt.cm.binary)\n",
    "    axs[i].set_xticks([]); axs[i].set_yticks([]); axs[i].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model using all data (not in batches)\n",
    "val_loss, val_acc = model.evaluate(val_images, val_labels, verbose=2)\n",
    "\n",
    "fig,ax = plt.subplots(nrows=2,ncols=1,figsize=(12,12))\n",
    "fs_L, fs_M, fs_S = 18, 16, 14\n",
    "ax[0].plot(history.history['categorical_accuracy'], label='train')\n",
    "ax[0].plot(history.history['val_categorical_accuracy'], label='validation')\n",
    "ax[0].set_xlabel('Epoch', fontsize=fs_M)\n",
    "ax[0].set_ylabel('Accuracy', fontsize=fs_M)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=fs_S)\n",
    "ax[0].set_title('Final mean validation accuracy: {}'.format(val_acc), fontsize=fs_M)\n",
    "ax[0].set_xticks(range(0,5))\n",
    "ax[0].legend(loc='lower right', fontsize=fs_M)\n",
    "\n",
    "ax[1].plot(history.history['loss'], label='train')\n",
    "ax[1].plot(history.history['val_loss'], label='validation')\n",
    "ax[1].set_xlabel('Epoch', fontsize=fs_M)\n",
    "ax[1].set_ylabel('Loss', fontsize=fs_M)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=fs_S)\n",
    "ax[1].set_xticks(range(0,5))\n",
    "ax[1].legend(loc='upper right', fontsize=fs_M)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using until now unseen data\n",
    "predicted_prob = model.predict(test_images)\n",
    "predictions = np.argmax(predicted_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = tf.math.confusion_matrix(labels=np.argmax(test_labels, axis=1), predictions=predictions, num_classes=num_classes)\n",
    "print('Confusion Matrix: ', conf_matrix)\n",
    "\n",
    "# Original code:\n",
    "#sess = tf.compat.v1.InteractiveSession()\n",
    "#conf_matrix = confusion.eval(session=sess)     # Needs update to new TF...\n",
    "#sess.close()\n",
    "\n",
    "# Accuracy score for inference\n",
    "error_rate = (np.sum(conf_matrix)-np.sum(np.diag(conf_matrix))) / np.sum(np.diag(conf_matrix))\n",
    "inf_acc = 1-error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 16}, fmt=\"d\", linewidths=.5, square=True, cbar=False, cmap='Greens')\n",
    "ax.invert_yaxis()\n",
    "plt.ylabel('Actual label', fontsize=16)\n",
    "plt.xlabel('Predicted label', fontsize=16)\n",
    "plt.xticks(fontsize=14); plt.yticks(fontsize=14)\n",
    "plt.title('Accuracy Score: {:.4f}'.format(inf_acc), fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print('Correct: {0}/{1}'.format(np.sum(np.diag(conf_matrix)),np.sum(conf_matrix)))\n",
    "print('Wrong: {0}/{1}'.format((np.sum(conf_matrix)-np.sum(np.diag(conf_matrix))),np.sum(conf_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctly predicted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 10 images that were correctly predicted\n",
    "correct_idx = np.where(predictions == np.argmax(test_labels, axis=1))[0]\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(15, 7))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].imshow(test_images[correct_idx[i],:,:,0], cmap=plt.cm.binary)\n",
    "    axs[i].set_title('predicted: {} \\n actual: {}'.format(predictions[correct_idx[i]], np.argmax(test_labels, axis=1)[correct_idx[i]]), fontsize=18)\n",
    "    axs[i].set_xticks([]); axs[i].set_yticks([]); axs[i].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrongly predicted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 10 images that were wrongly predicted:\n",
    "wrong_idx = np.where(predictions != np.argmax(test_labels, axis=1))[0]\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(15, 7))\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    axs[i].imshow(test_images[wrong_idx[i],:,:,0], cmap=plt.cm.binary)\n",
    "    axs[i].set_title('predicted: {} \\n actual: {}'.format(predictions[wrong_idx[i]], np.argmax(test_labels, axis=1)[wrong_idx[i]]), fontsize=18)\n",
    "    axs[i].set_xticks([]); axs[i].set_yticks([]); axs[i].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note:\n",
    "\n",
    "This reflects a very simple and crude CNN model. Since MNIST is a relatively straightforward dataset, using a \"larger\" (i.e. full scale) CNN model, you should expect even better results! However, the real test is, if you can apply the above approach to another more complicated dataset.\n",
    "\n",
    "Also, as the size of the problem grows (larger images!!!), the use of GPUs becomes necessary. They are typically a factor 50-100 faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
